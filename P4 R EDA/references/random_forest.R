## What drives demand for DC bike rentals?
```{r, echo=FALSE, include=FALSE}

library(ggplot2)
library(lubridate)
library(plyr)
library(randomForest)
library(scales)
```
```{r, echo=FALSE, collapse=TRUE}

set.seed(1)

train <- read.csv("../input/train.csv")
test  <- read.csv("../input/test.csv")

extract_features <- function(data) {
  features <- c("holiday",
                "workingday",
                "weather",
                "temp",
                "humidity",
                "windspeed",
                "hour",
                "month")
  # reminder - take atemp-temp difference
  data$hour  <- hour(ymd_hms(data$datetime))
  data$month <- month(ymd_hms(data$datetime))
  return(data[,features])
}

# Train a model across all the training data and plot the variable importance

sample_locs <- sample.int(nrow(train))
cutoff <- as.integer(nrow(train)*0.7)
internal_train <- train[sample_locs[1:cutoff],]
internal_valid <- train[sample_locs[(cutoff+1):nrow(train)],]

features <- extract_features(internal_train)
```
We have information on weather, date, time, and the hourly bike rentals in Washington DC. We want to determine what factors predict bike rental demand, as well as how these factors impact the demand.

In order to do this, we'll train a random forest model on the available data & then interrogate the model to figure out what it's learned.
```{r}
rf <- randomForest(features, internal_train$count, ntree=100, importance=TRUE)
```
From this model, we calculate the relative importance of the input features.
```{r, echo=FALSE}

imp <- importance(rf, type=1)
featureImportance <- data.frame(Feature=row.names(imp), Importance=imp[,1])

ggplot(featureImportance, aes(x=reorder(Feature, Importance), y=Importance)) +
  geom_bar(stat="identity", fill="#53cfff") +
  coord_flip() + 
  theme_light(base_size=16) +
  xlab("") + 
  ylab("Relative Importance") +
  theme(plot.title   = element_text(size=18),
        strip.text.x = element_blank(),
        axis.text.x  = element_blank(),
        axis.ticks.x = element_blank())

```

We can use the random forest model to generate partial plots of the individual features, enabling us to visualize what the model's learned about them.

Time and temperature are the two factors that have the greatest impact on DC bike demand.
```{r, echo=FALSE}

partials <- data.frame()

for (i in seq_along(names(features))) {
partial <- partialPlot(rf, features, names(features)[i], plot=FALSE)
xt <- rescale(partial$x)
partials <- rbind(partials, data.frame(x=partial$x, xt=xt, y=partial$y, feature=names(features)[i]))
}

ranges <- ddply(partials, "feature", function(d) {
r <- range(d$y)
data.frame(feature=d$feature[1], range=r[2]-r[1])
})

features_to_plot <- ranges[ranges$range>0.05*max(ranges$range),"feature"]

ggplot(partials[partials$feature %in% features_to_plot,], aes(x=xt, y=y, color=feature)) +
geom_line(size=2) +
theme_light(base_size=16) +
xlab("Feature Range (Min to Max)") +
ylab("Hourly Bike Rentals") 

```
When we make predictions on our internal validation set, we see that the correlations are high.
```{r, echo=FALSE}
valid_features <- extract_features(internal_valid)
valid_features$Predictions <- predict(rf, extract_features(internal_valid))
valid_features$Actuals     <- internal_valid$count

ggplot(valid_features, aes(x=Actuals, y=Predictions)) +
geom_point() + 
theme_light(base_size=16) +
xlab("Actual Hourly Bike Rentals") +
ylab("Predicted Hourly Bike Rentals") +
ggtitle(paste0("Correlation: ", round(cor(valid_features$Actuals, valid_features$Predictions), 3)))
```
